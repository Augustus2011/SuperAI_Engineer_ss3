{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport cv2\nimport torch \nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass Config:\n    DATA_DIR = '/kaggle/input/phuket-location/images/images'\n    CSV_PATH = os.path.join('/kaggle/input/phuket-location', 'train.csv')\n    CSV_PATH_TEST = os.path.join('/kaggle/input/phuket-location', 'test.csv')\n    train_batch_size = 10\n    val_batch_size = 10\n    num_workers = 1\n    image_size = 224\n    output_dim = 512\n    hidden_dim = 1024\n    input_dim = 3\n    epochs = 20\n    lr = 1e-4\n    num_of_classes = 15\n    pretrained = True\n    model_name = 'resnet101'\n    seed = 42\n    \nConfig = Config()","metadata":{"execution":{"iopub.status.busy":"2023-04-22T23:47:46.331183Z","iopub.execute_input":"2023-04-22T23:47:46.332201Z","iopub.status.idle":"2023-04-22T23:47:46.340387Z","shell.execute_reply.started":"2023-04-22T23:47:46.332161Z","shell.execute_reply":"2023-04-22T23:47:46.338890Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# DataSet","metadata":{}},{"cell_type":"code","source":"from torchvision import transforms\nfrom torchvision.transforms import ToTensor\n\ntrain_transform = transforms.Compose([\n        transforms.ToPILImage(),\n        # transforms.RandomRotation(10),         # rotate +/- 10 degrees\n        # transforms.RandomHorizontalFlip(),     # reverse 50% of images\n        # transforms.Resize(Config.image_size, Config.image_size),   ##### resize shortest side to 224 pixels\n        # transforms.CenterCrop(224),            ##### crop longest side to 224 pixels at center\n    \n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ])\n\nval_transform = transforms.Compose([\n        transforms.ToPILImage(),\n        # transforms.RandomHorizontalFlip(),     # reverse 50% of images\n        # transforms.Resize(Config.image_size, Config.image_size),   ##### resize shortest side to 224 pixels\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ])\n","metadata":{"execution":{"iopub.status.busy":"2023-04-22T23:47:49.124300Z","iopub.execute_input":"2023-04-22T23:47:49.124689Z","iopub.status.idle":"2023-04-22T23:47:49.131573Z","shell.execute_reply.started":"2023-04-22T23:47:49.124630Z","shell.execute_reply":"2023-04-22T23:47:49.130355Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# Train-Val Split","metadata":{}},{"cell_type":"code","source":"# # Split train into train and val\ndf_train = pd.read_csv('/kaggle/input/phuket-location/train.csv')\n\ndf_trainn, df_val = train_test_split(df_train, test_size=0.01, random_state=123)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T23:47:49.899887Z","iopub.execute_input":"2023-04-22T23:47:49.900628Z","iopub.status.idle":"2023-04-22T23:47:49.913859Z","shell.execute_reply.started":"2023-04-22T23:47:49.900589Z","shell.execute_reply":"2023-04-22T23:47:49.912813Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom torch.utils.data import Dataset\n\n\ndef img_path_from_id(id):\n    img_path = os.path.join(Config.DATA_DIR, 'train', f'{id}')\n    return img_path\n\ndef img_path_from_idd(id):\n    img_path = os.path.join(Config.DATA_DIR, 'test', f'{id}')\n    return img_path\n\n\nclass ImageDataset(Dataset):\n    def __init__(self):\n        self.df = df_trainn\n        \n        self.landmark_id_encoder = preprocessing.LabelEncoder()\n        #self.df['label'] = self.landmark_id_encoder.fit_transform(self.df['label'])\n        self.df['path'] = self.df['id'].apply(img_path_from_id)\n        self.paths = self.df['path'].values\n        self.ids = self.df['id'].values\n        self.landmark_ids = self.df['label'].values\n        self.transform = train_transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        path, id, landmark_id = self.paths[idx], self.ids[idx], self.landmark_ids[idx]\n        \n        img = cv2.imread(path)\n        img = cv2.resize(img, (Config.image_size , Config.image_size))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n#         img = Image.open(path)\n#         img = Image.fromarray(img)\n        # img = self.transform(img)\n        \n        if self.transform:\n            img = self.transform(img)\n        \n        #labels = torch.tensor(self.landmark_ids[idx])  \n        \n        return img.to(device), landmark_id, id ","metadata":{"execution":{"iopub.status.busy":"2023-04-22T23:47:51.824355Z","iopub.execute_input":"2023-04-22T23:47:51.825065Z","iopub.status.idle":"2023-04-22T23:47:51.835962Z","shell.execute_reply.started":"2023-04-22T23:47:51.825024Z","shell.execute_reply":"2023-04-22T23:47:51.834687Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"class ArcFace(nn.Module):\n    def __init__(self, in_features, out_features, scale_factor=64.0, \n                 margin=0.50, \n                 criterion=None):\n        super(ArcFace, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n\n        if criterion:\n            self.criterion = criterion\n        else:\n            self.criterion = nn.CrossEntropyLoss()\n\n        self.margin = margin\n        self.scale_factor = scale_factor\n\n        self.weight = nn.Parameter(\n            torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n\n    def forward(self, input, label):\n        # input is not l2 normalized\n        # cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        cosine = F.linear(F.normalize(input.to(device)), F.normalize(self.weight.to(device)))\n        \n        # F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n\n        phi = cosine * self.cos_m - sine * self.sin_m\n        phi = phi.type(cosine.type())\n        phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n\n        one_hot = torch.zeros(cosine.size(), device= input.device).to(device)\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n\n        logit = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        logit *= self.scale_factor\n\n        #loss = self.criterion(logit, label) \n        \n        loss = self.criterion(logit.to(device), label.to(device))\n\n        return loss, logit ","metadata":{"execution":{"iopub.status.busy":"2023-04-22T23:47:52.219190Z","iopub.execute_input":"2023-04-22T23:47:52.220147Z","iopub.status.idle":"2023-04-22T23:47:52.231325Z","shell.execute_reply.started":"2023-04-22T23:47:52.220100Z","shell.execute_reply":"2023-04-22T23:47:52.230178Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"# GeM","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6, requires_grad=False):\n        super(GeM, self).__init__()\n        self.p = nn.Parameter(torch.ones(1)*p, requires_grad=requires_grad).to(device)\n        self.eps = eps\n\n    def forward(self, x):\n        return self.gem(x.to(device), p=self.p, eps=self.eps)\n    \n\n    def gem(self, x, p=3, eps=1e-6):\n        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p).to(device)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'","metadata":{"execution":{"iopub.status.busy":"2023-04-22T23:47:53.802417Z","iopub.execute_input":"2023-04-22T23:47:53.803960Z","iopub.status.idle":"2023-04-22T23:47:53.813003Z","shell.execute_reply.started":"2023-04-22T23:47:53.803906Z","shell.execute_reply":"2023-04-22T23:47:53.811930Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# MultiAtrous","metadata":{}},{"cell_type":"code","source":"import timm\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom pytorch_lightning import LightningModule\nimport torchmetrics\n\nclass MultiAtrous(nn.Module):\n    def __init__(self, in_channel, out_channel, size, dilation_rates=[3, 6, 9]):\n        super().__init__()\n        self.dilated_convs = [\n            nn.Conv2d(in_channel, int(out_channel/4),\n                      kernel_size=3, dilation=rate, padding=rate)\n            for rate in dilation_rates\n        ]\n        self.gap_branch = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channel, int(out_channel/4), kernel_size=1),\n            nn.ReLU(),\n            nn.Upsample(size=(size, size), mode='bilinear')\n        )\n        self.dilated_convs.append(self.gap_branch)\n        self.dilated_convs = nn.ModuleList(self.dilated_convs)\n\n    def forward(self, x):\n        local_feat = []\n        for dilated_conv in self.dilated_convs:\n            local_feat.append(dilated_conv(x))\n        local_feat = torch.cat(local_feat, dim=1)\n        return local_feat\n\n\nclass DolgLocalBranch(nn.Module):\n    def __init__(self, in_channel, out_channel, hidden_channel=2048):\n        super().__init__()\n        self.multi_atrous = MultiAtrous(in_channel, hidden_channel, size=int(Config.image_size/8))\n        self.conv1x1_1 = nn.Conv2d(hidden_channel, out_channel, kernel_size=1)\n        self.conv1x1_2 = nn.Conv2d(\n            out_channel, out_channel, kernel_size=1, bias=False)\n        self.conv1x1_3 = nn.Conv2d(out_channel, out_channel, kernel_size=1)\n\n        self.relu = nn.ReLU()\n        self.bn = nn.BatchNorm2d(out_channel)\n        self.softplus = nn.Softplus()\n        \n        \n    def forward(self, x):\n        local_feat = self.multi_atrous(x)\n\n        local_feat = self.conv1x1_1(local_feat)\n        local_feat = self.relu(local_feat)\n        local_feat = self.conv1x1_2(local_feat)\n        local_feat = self.bn(local_feat)\n\n        attention_map = self.relu(local_feat)\n        attention_map = self.conv1x1_3(attention_map)\n        attention_map = self.softplus(attention_map)\n\n        local_feat = F.normalize(local_feat, p=2, dim=1)\n        local_feat = local_feat * attention_map\n\n        return local_feat\n\n\nclass OrthogonalFusion(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, local_feat, global_feat):\n        global_feat_norm = torch.norm(global_feat, p=2, dim=1)\n        projection = torch.bmm(global_feat.unsqueeze(1), torch.flatten(\n            local_feat, start_dim=2))\n        projection = torch.bmm(global_feat.unsqueeze(\n            2), projection).view(local_feat.size())\n        projection = projection / \\\n            (global_feat_norm * global_feat_norm).view(-1, 1, 1, 1)\n        orthogonal_comp = local_feat - projection\n        global_feat = global_feat.unsqueeze(-1).unsqueeze(-1).to(device)\n        \n        return torch.cat([global_feat.expand(orthogonal_comp.size()), orthogonal_comp], dim=1)\n\n\nclass DolgNet(LightningModule):\n    def __init__(self, input_dim, hidden_dim, output_dim, num_of_classes):\n        super().__init__()\n        self.cnn = timm.create_model('tv_resnet101',\n                    pretrained=True,\n                    features_only=True,\n                    in_chans=input_dim,\n                    out_indices=(2, 3))\n        \n        \n        self.orthogonal_fusion = OrthogonalFusion()\n        \n        self.local_branch = DolgLocalBranch(512, hidden_dim)\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.fc_1 = nn.Linear(1024, hidden_dim)\n        self.fc_2 = nn.Linear(int(2*hidden_dim), output_dim)\n        \n        self.criterion = ArcFace( in_features  = output_dim,\n                                  out_features = num_of_classes,\n                                  scale_factor=30,\n                                  margin=0.15,\n                                  criterion=nn.CrossEntropyLoss())\n        \n        self.lr = Config.lr\n        \n        self.accuracy = torchmetrics.Accuracy(task= \"multiclass\", \n                                 num_classes= Config.num_of_classes)\n\n        \n    def forward(self, x):\n        output = self.cnn(x)\n\n        local_feat = self.local_branch(output[0])   # ,hidden_channel,16,16\n        global_feat = self.fc_1(GeM()(output[1]).squeeze())  # ,1024\n\n        feat = self.orthogonal_fusion(local_feat, global_feat)\n        feat = self.gap(feat).squeeze()\n        feat = self.fc_2(feat)\n        \n        return feat #out\n\n    def training_step(self, batch, batch_idx):\n        img, label, _ = batch\n        embd = self(img).to(device)\n        \n        loss, logits = self.criterion(embd, label)\n\n        \n        return loss.to(device)\n\n    def configure_optimizers(self):\n        optimizer = optim.SGD(self.parameters(), lr=self.lr,\n                              momentum=0.9, weight_decay=1e-5)\n        scheduler = scheduler = optim.lr_scheduler.CosineAnnealingLR(\n                                      optimizer, T_max=1000)\n        \n        return [optimizer], [scheduler]\n\n    def train_dataloader(self):\n        dataset = ImageDataset()\n        \n        return DataLoader(dataset, \n                          batch_size= Config.train_batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T23:47:54.832859Z","iopub.execute_input":"2023-04-22T23:47:54.833286Z","iopub.status.idle":"2023-04-22T23:47:55.142071Z","shell.execute_reply.started":"2023-04-22T23:47:54.833248Z","shell.execute_reply":"2023-04-22T23:47:55.140958Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"from pytorch_lightning.utilities.seed import seed_everything\nfrom pytorch_lightning import Trainer\nimport multiprocessing as mp\n\nseed_everything(Config.seed)\n\nmodel = DolgNet(\n        input_dim  = Config.input_dim,\n        hidden_dim = Config.hidden_dim,\n        output_dim = Config.output_dim,\n        num_of_classes = Config.num_of_classes)\n\nmodel = model.to(device)\n\ntrainer = Trainer(gpus=1, max_epochs = Config.epochs)\n\ntrainer.fit(model)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T23:47:55.608664Z","iopub.execute_input":"2023-04-22T23:47:55.609027Z","iopub.status.idle":"2023-04-23T00:20:21.932164Z","shell.execute_reply.started":"2023-04-22T23:47:55.608994Z","shell.execute_reply":"2023-04-23T00:20:21.931056Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/seed.py:48: LightningDeprecationWarning: `pytorch_lightning.utilities.seed.seed_everything` has been deprecated in v1.8.0 and will be removed in v2.0.0. Please use `lightning_fabric.utilities.seed.seed_everything` instead.\n  \"`pytorch_lightning.utilities.seed.seed_everything` has been deprecated in v1.8.0 and will be\"\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd881c14b2cb402e87de4d9bfbe1f332"}},"metadata":{}}]},{"cell_type":"code","source":"# Specify a path\nPATH = \"/kaggle/working/DOLG_20__1024_sub.pt\"\n\n# # # Save\ntorch.save(model, PATH)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:22:25.385585Z","iopub.execute_input":"2023-04-23T00:22:25.386367Z","iopub.status.idle":"2023-04-23T00:22:26.019036Z","shell.execute_reply.started":"2023-04-23T00:22:25.386327Z","shell.execute_reply":"2023-04-23T00:22:26.017915Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Load\nmodell = torch.load(PATH)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:22:27.028336Z","iopub.execute_input":"2023-04-23T00:22:27.029047Z","iopub.status.idle":"2023-04-23T00:22:27.391154Z","shell.execute_reply.started":"2023-04-23T00:22:27.029008Z","shell.execute_reply":"2023-04-23T00:22:27.390024Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"# Extract Embedded Vector","metadata":{}},{"cell_type":"code","source":"# Validation\nimport tqdm\nfrom tqdm import tqdm\n\nclass ImageValDataset(Dataset):\n    def __init__(self):\n        self.df = df_val\n        \n        #self.landmark_id_encoder = preprocessing.LabelEncoder()\n        \n        self.df['path'] = self.df['id'].apply(img_path_from_id)\n        \n        self.paths = self.df['path'].values\n        self.ids = self.df['id'].values\n        self.landmark_ids = self.df['label'].values\n        self.transform = train_transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        path, id, landmark_id = self.paths[idx], self.ids[idx], self.landmark_ids[idx]\n        \n        img = cv2.imread(path)\n        img = cv2.resize(img, (Config.image_size , Config.image_size))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        \n        if self.transform:\n            img = self.transform(img)\n        \n        #labels = torch.tensor(self.landmark_ids[idx])  \n        \n        return img.to(device), landmark_id, id #str #torch.permute(img, (2, 0, 1))","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:22:30.266764Z","iopub.execute_input":"2023-04-23T00:22:30.267409Z","iopub.status.idle":"2023-04-23T00:22:30.276392Z","shell.execute_reply.started":"2023-04-23T00:22:30.267367Z","shell.execute_reply":"2023-04-23T00:22:30.275252Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"train_set = ImageDataset()\ntrain_loader = DataLoader(train_set , batch_size= Config.train_batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:22:30.740562Z","iopub.execute_input":"2023-04-23T00:22:30.741721Z","iopub.status.idle":"2023-04-23T00:22:30.754178Z","shell.execute_reply.started":"2023-04-23T00:22:30.741641Z","shell.execute_reply":"2023-04-23T00:22:30.752936Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"Train = []\nmodell = modell.to(device)\n\nfor batch in tqdm(train_loader):\n    inputs, y_train, _ = batch\n    Emv_v = modell(inputs).to(device)\n    Emv_v = Emv_v.cpu().detach().numpy()\n    Emv_v = Emv_v.astype(np.float32)\n    \n    Train.append((Emv_v, y_train.numpy()))","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:22:33.061400Z","iopub.execute_input":"2023-04-23T00:22:33.061884Z","iopub.status.idle":"2023-04-23T00:23:35.584952Z","shell.execute_reply.started":"2023-04-23T00:22:33.061846Z","shell.execute_reply":"2023-04-23T00:23:35.583841Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"100%|██████████| 270/270 [01:02<00:00,  4.32it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"val_set = ImageValDataset()\nval_loader = DataLoader(val_set , batch_size = Config.train_batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:23:35.587104Z","iopub.execute_input":"2023-04-23T00:23:35.587846Z","iopub.status.idle":"2023-04-23T00:23:35.595046Z","shell.execute_reply.started":"2023-04-23T00:23:35.587798Z","shell.execute_reply":"2023-04-23T00:23:35.593970Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"Val = []\nmodell = modell.to(device)\n\nfor batch in tqdm(val_loader):\n    inputs, y_val, _ = batch\n    Emv_v = modell(inputs).to(device)\n    Emv_v = Emv_v.cpu().detach().numpy()\n    Emv_v = Emv_v.astype(np.float32)\n    \n    Val.append((Emv_v, y_val.numpy()))","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:23:35.596535Z","iopub.execute_input":"2023-04-23T00:23:35.597064Z","iopub.status.idle":"2023-04-23T00:23:36.210923Z","shell.execute_reply.started":"2023-04-23T00:23:35.597025Z","shell.execute_reply":"2023-04-23T00:23:36.209844Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"100%|██████████| 3/3 [00:00<00:00,  5.05it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"Train_Data  = []\nTrain_Lable = []\n\nVal_Data = []\nVal_Lable = []\n\nfor pair in Train:\n    for batch in pair[0]:\n        Train_Data.append(batch)\n    #print(pair[1])\n    for batch in pair[1]:\n        Train_Lable.append(batch)\n\nfor pair in Val:\n    for batch in pair[0]:\n        Val_Data.append(batch)\n    for batch in pair[1]:\n        Val_Lable.append(batch)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:23:42.186779Z","iopub.execute_input":"2023-04-23T00:23:42.187844Z","iopub.status.idle":"2023-04-23T00:23:42.198593Z","shell.execute_reply.started":"2023-04-23T00:23:42.187801Z","shell.execute_reply":"2023-04-23T00:23:42.197536Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# Evaluating \nimport math\n\ndef Evaluating(Train_Data, Train_Lable, Val_Data, Val_Lable):\n    Prd_Val = []\n    for num1, vec_val in tqdm(enumerate(Val_Data)):\n        Check = []\n        \n        for num2, vec_train in enumerate(Train_Data):\n            dist = np.linalg.norm(vec_val - vec_train)\n            Check.append(dist)\n\n        Prd_Val.append(Train_Lable[np.argmin(Check)])\n        \n        Check = []\n        \n        if num1  == 20: print(Prd_Val) \n        \n        ACC = np.sum(np.array(Prd_Val) == np.array(Val_Lable))/len(Val_Data)\n        \n    return print(ACC), Prd_Val\n                \nACC, Prd_Val = Evaluating(Train_Data, Train_Lable, Val_Data, Val_Lable)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:23:44.705830Z","iopub.execute_input":"2023-04-23T00:23:44.706460Z","iopub.status.idle":"2023-04-23T00:23:45.270259Z","shell.execute_reply.started":"2023-04-23T00:23:44.706418Z","shell.execute_reply":"2023-04-23T00:23:45.269131Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"0it [00:00, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n28it [00:00, 51.03it/s]","output_type":"stream"},{"name":"stdout","text":"[2, 13, 7, 0, 9, 10, 12, 2, 12, 3, 0, 0, 4, 6, 8, 5, 5, 2, 1, 14, 4]\n1.0\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# 94.67 epoch 7\n# 94.31 epoch 5\n# 93.76 epoch 2\n# 94.86 epoch 12\n\n# 93.21 epoch 2 #448\n# 93.02 epoch 2 #440 ","metadata":{"execution":{"iopub.status.busy":"2023-04-22T22:52:51.878233Z","iopub.execute_input":"2023-04-22T22:52:51.879247Z","iopub.status.idle":"2023-04-22T22:52:51.883699Z","shell.execute_reply.started":"2023-04-22T22:52:51.879206Z","shell.execute_reply":"2023-04-22T22:52:51.882611Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"class ImageTestDataset(Dataset):\n    def __init__(self):\n        self.df = pd.read_csv(Config.CSV_PATH_TEST)\n        \n        self.df['path'] = self.df['id'].apply(img_path_from_idd)\n        self.paths = self.df['path'].values\n        self.ids = self.df['id'].values\n        self.transform = train_transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        path, id = self.paths[idx], self.ids[idx]\n        \n        img = cv2.imread(path)\n        img = cv2.resize(img, (Config.image_size , Config.image_size))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if self.transform:\n            img = self.transform(img)\n        \n        return img.to(device), id\n    \ntest_set = ImageTestDataset()\ntest_loader = DataLoader(test_set , batch_size=10)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:23:49.050354Z","iopub.execute_input":"2023-04-23T00:23:49.051296Z","iopub.status.idle":"2023-04-23T00:23:49.067454Z","shell.execute_reply.started":"2023-04-23T00:23:49.051240Z","shell.execute_reply":"2023-04-23T00:23:49.066248Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"Test_Data  = []\n\nmodell = modell.to(device)\n\nfor batch in tqdm(test_loader):\n    inputs, _ = batch\n    Emv_v = modell(inputs).to(device)\n    Emv_v = Emv_v.cpu().detach().numpy()\n    Emv_v = Emv_v.astype(np.float32)\n    \n    for vec in Emv_v:   \n        Test_Data.append(vec)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:23:51.010372Z","iopub.execute_input":"2023-04-23T00:23:51.011114Z","iopub.status.idle":"2023-04-23T00:24:09.876289Z","shell.execute_reply.started":"2023-04-23T00:23:51.011066Z","shell.execute_reply":"2023-04-23T00:24:09.875179Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stderr","text":"100%|██████████| 75/75 [00:18<00:00,  3.98it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"Prd_Test = []\n\nfor num1, vec_tst in tqdm(enumerate(Test_Data)):\n    Check = []\n    for num2, vec_train in enumerate(Train_Data):\n        dist = np.linalg.norm(vec_tst - vec_train)\n        Check.append(dist)\n        \n    Prd_Test.append(Train_Lable[np.argmin(Check)])\n    Check = []","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:24:12.498052Z","iopub.execute_input":"2023-04-23T00:24:12.498442Z","iopub.status.idle":"2023-04-23T00:24:27.538979Z","shell.execute_reply.started":"2023-04-23T00:24:12.498410Z","shell.execute_reply":"2023-04-23T00:24:27.537858Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"750it [00:15, 49.90it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ML models","metadata":{}},{"cell_type":"code","source":"import statistics as stat\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.cluster import KMeans\nimport lightgbm as lgb\n\n\n# Normalize data\ninput_train = np.array(Train_Data)\nnormalized_train = preprocessing.normalize(input_train, norm='l2')\n\ninput_val = np.array(Val_Data)\nnormalized_val = preprocessing.normalize(input_val, norm='l2')\n\ninput_test = np.array(Test_Data)\nnormalized_test = preprocessing.normalize(input_test, norm='l2')","metadata":{"execution":{"iopub.status.busy":"2023-04-22T22:53:27.041689Z","iopub.execute_input":"2023-04-22T22:53:27.042578Z","iopub.status.idle":"2023-04-22T22:53:28.963713Z","shell.execute_reply.started":"2023-04-22T22:53:27.042535Z","shell.execute_reply":"2023-04-22T22:53:28.962580Z"},"trusted":true},"execution_count":48,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"# SVM","metadata":{}},{"cell_type":"code","source":"# Training\nSVM = svm.SVC()\nSVM.fit(normalized_train, Train_Lable)\n\n# Evaluating\nnp.sum(SVM.predict(normalized_val) == Val_Lable)/len(Val_Lable)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T22:53:28.965111Z","iopub.execute_input":"2023-04-22T22:53:28.966152Z","iopub.status.idle":"2023-04-22T22:53:29.618880Z","shell.execute_reply.started":"2023-04-22T22:53:28.966112Z","shell.execute_reply":"2023-04-22T22:53:29.617698Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"0.9394495412844037"},"metadata":{}}]},{"cell_type":"markdown","source":"# KNN","metadata":{}},{"cell_type":"code","source":"# Training\nKNN = KNeighborsClassifier(n_neighbors = 15)\nKNN.fit(normalized_train, Train_Lable)\n\n# Evaluating\nnp.sum(KNN.predict(normalized_val) == Val_Lable)/len(Val_Lable)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T22:53:29.620390Z","iopub.execute_input":"2023-04-22T22:53:29.620849Z","iopub.status.idle":"2023-04-22T22:53:29.730337Z","shell.execute_reply.started":"2023-04-22T22:53:29.620811Z","shell.execute_reply":"2023-04-22T22:53:29.728730Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"0.9357798165137615"},"metadata":{}}]},{"cell_type":"markdown","source":"# Ridge Classifier","metadata":{}},{"cell_type":"code","source":"# Training\nRC = RidgeClassifier().fit(normalized_train, Train_Lable)\n\n# Evaluating\nnp.sum(RC.predict(normalized_val) == Val_Lable)/len(Val_Lable)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T22:53:29.736970Z","iopub.execute_input":"2023-04-22T22:53:29.740415Z","iopub.status.idle":"2023-04-22T22:53:29.835378Z","shell.execute_reply.started":"2023-04-22T22:53:29.740354Z","shell.execute_reply":"2023-04-22T22:53:29.834044Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"0.9357798165137615"},"metadata":{}}]},{"cell_type":"markdown","source":"# LightGBM","metadata":{}},{"cell_type":"code","source":"# It'll take a while to run this cell\n\n# # Training\n# Lgbm = lgb.LGBMClassifier()\n# Lgbm.fit(normalized_train, Train_Lable)\n\n# # Evaluating\n# np.sum(Lgbm.predict(normalized_val) == Val_Lable)/len(Val_Lable)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T22:53:29.841171Z","iopub.execute_input":"2023-04-22T22:53:29.844713Z","iopub.status.idle":"2023-04-22T22:53:29.852877Z","shell.execute_reply.started":"2023-04-22T22:53:29.844656Z","shell.execute_reply":"2023-04-22T22:53:29.851411Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"# Kmeans","metadata":{}},{"cell_type":"code","source":"kmeans = KMeans(n_clusters=15, random_state=0).fit(np.array(list(normalized_train) + \n                                                            list(normalized_val) ))\nPRD_TRN = kmeans.predict(normalized_train)\nPRD_VAL = kmeans.predict(normalized_val)\n\nCluster = [[] for i in range(15)]\nfor idx, clss in enumerate(Train_Lable) :\n    Cluster[clss].append(PRD_TRN[idx])\n    \nClss = [stat.mode(clss) for clss in Cluster] \nvall = []\nfor prd_val in PRD_VAL:\n    vall.append(Clss.index(prd_val))\n    \n# # Evaluating\nnp.sum(np.array(vall) == Val_Lable)/len(Val_Lable)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T22:53:29.859480Z","iopub.execute_input":"2023-04-22T22:53:29.861626Z","iopub.status.idle":"2023-04-22T22:53:31.665766Z","shell.execute_reply.started":"2023-04-22T22:53:29.861575Z","shell.execute_reply":"2023-04-22T22:53:31.664108Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"0.9302752293577982"},"metadata":{}}]},{"cell_type":"markdown","source":"# Ensemble Method","metadata":{}},{"cell_type":"code","source":"END = []\nSVM_PRD = SVM.predict(normalized_val)\nRC_PRD  = RC.predict(normalized_val)\nKNN_PRD = KNN.predict(normalized_val)\n\nfor idx in range(len(Val_Lable)):\n    try:\n        END.append(stat.mode([Prd_Val[idx], SVM_PRD[idx],  \n                              RC_PRD[idx], KNN_PRD[idx] ]))\n        \n    except: \n        END.append(Prd_Val[idx])\n        \n# Evaluating\nnp.sum(np.array(END) == Val_Lable)/len(Val_Lable)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T22:53:31.672226Z","iopub.execute_input":"2023-04-22T22:53:31.675930Z","iopub.status.idle":"2023-04-22T22:53:32.096446Z","shell.execute_reply.started":"2023-04-22T22:53:31.675864Z","shell.execute_reply":"2023-04-22T22:53:32.095218Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"0.9431192660550459"},"metadata":{}}]},{"cell_type":"markdown","source":"# Episodic Predicting","metadata":{}},{"cell_type":"code","source":"TEST_QUERY = [[] for i in range(15)]\n\nfor num, vec_train in enumerate(Train_Data):\n    TEST_QUERY[Train_Lable[num]] = TEST_QUERY[Train_Lable[num]] + [vec_train]\n    \nLen_Query = [len(Q) for Q in TEST_QUERY]","metadata":{"execution":{"iopub.status.busy":"2023-04-22T22:53:32.098219Z","iopub.execute_input":"2023-04-22T22:53:32.099384Z","iopub.status.idle":"2023-04-22T22:53:32.112161Z","shell.execute_reply.started":"2023-04-22T22:53:32.099344Z","shell.execute_reply":"2023-04-22T22:53:32.110476Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"def EpisodeGen():\n    Episode_index = [np.random.randint(len(TEST_QUERY[idx]), size = min(Len_Query))\n                     for idx in range(15)]\n\n    Episode = [[] for idx in range(min(Len_Query))]\n\n    for i in range(min(Len_Query)):\n        for j in range(15):\n            Episode[i] = Episode[i] + [TEST_QUERY[j][Episode_index[j][i]]]\n            \n    return Episode","metadata":{"execution":{"iopub.status.busy":"2023-04-22T22:53:32.114497Z","iopub.execute_input":"2023-04-22T22:53:32.115309Z","iopub.status.idle":"2023-04-22T22:53:32.125145Z","shell.execute_reply.started":"2023-04-22T22:53:32.115272Z","shell.execute_reply":"2023-04-22T22:53:32.123534Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"EPD_VAL = []\nCheckk = []\nBOX = []\n\nfor vec_val in tqdm(Val_Data):\n    Episode = EpisodeGen() \n    for epd in Episode :\n        for q_eq in epd:\n            dist = np.linalg.norm(vec_val - q_eq)\n            Checkk.append(dist)\n        BOX.append(np.argmin(Checkk))\n        Checkk = []\n        \n    EPD_VAL.append(stat.mode(BOX))   \n    BOX = []\n\n# Evaluate Episodic Predicting\nnp.sum(np.array(EPD_VAL) == Val_Lable)/len(Val_Lable)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T22:53:32.127461Z","iopub.execute_input":"2023-04-22T22:53:32.128734Z","iopub.status.idle":"2023-04-22T22:53:39.362296Z","shell.execute_reply.started":"2023-04-22T22:53:32.128696Z","shell.execute_reply":"2023-04-22T22:53:39.361383Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stderr","text":"100%|██████████| 545/545 [00:07<00:00, 75.64it/s]\n","output_type":"stream"},{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"0.9321100917431193"},"metadata":{}}]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"ID = list(pd.read_csv('/kaggle/input/phuket-location/submit.csv')['id'])\n\n# Create a DataFrame from the dictionary\ndf_submission = pd.DataFrame(data = {'id': ID, 'predict': Prd_Test})\ndf_submission.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:24:35.601523Z","iopub.execute_input":"2023-04-23T00:24:35.602798Z","iopub.status.idle":"2023-04-23T00:24:35.620637Z","shell.execute_reply.started":"2023-04-23T00:24:35.602753Z","shell.execute_reply":"2023-04-23T00:24:35.619688Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"                                     id  predict\n0  6a270f855c45a53c9cd29704e4ec1811.jpg        3\n1  2c76f54dd40ef6747cc2e179c091173c.jpg        1\n2  8a2ab1e2da3aa24c838d2644b8beca77.jpg        7\n3  5b9c7c23ec76a6becf9c9beb4c7bd5f5.jpg       13\n4  6cea6f1ff4873fbcaa4e2ec4e082e361.jpg        3","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>predict</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6a270f855c45a53c9cd29704e4ec1811.jpg</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2c76f54dd40ef6747cc2e179c091173c.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8a2ab1e2da3aa24c838d2644b8beca77.jpg</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5b9c7c23ec76a6becf9c9beb4c7bd5f5.jpg</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6cea6f1ff4873fbcaa4e2ec4e082e361.jpg</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_submission.to_csv('/kaggle/working/submission_Dolg3.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:24:39.076613Z","iopub.execute_input":"2023-04-23T00:24:39.077128Z","iopub.status.idle":"2023-04-23T00:24:39.091223Z","shell.execute_reply.started":"2023-04-23T00:24:39.077084Z","shell.execute_reply":"2023-04-23T00:24:39.090090Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"os.listdir('/kaggle/working/')","metadata":{"execution":{"iopub.status.busy":"2023-04-22T22:53:39.426182Z","iopub.execute_input":"2023-04-22T22:53:39.426522Z","iopub.status.idle":"2023-04-22T22:53:39.439707Z","shell.execute_reply.started":"2023-04-22T22:53:39.426488Z","shell.execute_reply":"2023-04-22T22:53:39.438730Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"['.virtual_documents',\n 'submission_Dolg1.csv',\n 'DOLG_10_512.pt',\n '__notebook_source__.ipynb',\n 'lightning_logs']"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}